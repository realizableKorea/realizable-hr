import os
import chromadb
from sentence_transformers import SentenceTransformer
from bs4 import BeautifulSoup
from transformers import AutoTokenizer, AutoModelForCausalLM
import markdown

# ğŸ”¹ ì„¤ì •
DOCS_PATH = "./realizable_markdown"
DB_PATH = "./chroma_db_gemma"
COLLECTION_NAME = "company_docs"
# EMBEDDING_MODEL_NAME = "upstage/SOLAR-embedding-1-large"  # or "all-mpnet-base-v2"
EMBEDDING_MODEL_NAME = "all-mpnet-base-v2"  # or "all-mpnet-base-v2"
LLM_MODEL_NAME = "google/gemma-2b"

# ğŸ”¹ ChromaDB ì´ˆê¸°í™”
chroma_client = chromadb.PersistentClient(path=DB_PATH)
collection = chroma_client.get_or_create_collection(name=COLLECTION_NAME)

# ğŸ”¹ ì„ë² ë”© ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME)

# ğŸ”¹ ë¬¸ì„œ ì „ì²˜ë¦¬ í•¨ìˆ˜
def extract_text_from_markdown(file_path):
    with open(file_path, "r", encoding="utf-8") as f:
        html = markdown.markdown(f.read())
        soup = BeautifulSoup(html, "html.parser")
        return soup.get_text()

# ğŸ”¹ ë¬¸ì„œ ì„ë² ë”© ë° ChromaDBì— ì¶”ê°€
def embed_all_documents():
    for filename in os.listdir(DOCS_PATH):
        if filename.endswith(".md"):
            file_path = os.path.join(DOCS_PATH, filename)
            text = extract_text_from_markdown(file_path)
            embedding = embedding_model.encode([text])[0]  # ë‹¨ì¼ ë¬¸ì„œ
            collection.add(
                documents=[text],
                embeddings=[embedding.tolist()],
                ids=[filename]
            )
    print("âœ… ëª¨ë“  Markdown ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")

# ğŸ”¹ LLM ì¤€ë¹„ (Gemma)
def load_llm():
    tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME)
    model = AutoModelForCausalLM.from_pretrained(LLM_MODEL_NAME, device_map="auto")
    return tokenizer, model

# ğŸ”¹ ì§ˆë¬¸ì— ë‹µë³€ ìƒì„±
def answer_question(user_query, top_k=3):
    # 1. ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    query_embedding = embedding_model.encode([user_query])[0]
    results = collection.query(
        query_embeddings=[query_embedding.tolist()],
        n_results=top_k
    )

    # 2. ê´€ë ¨ ë¬¸ì„œ ë‚´ìš© í•©ì¹˜ê¸°
    context = "\n\n".join(results['documents'][0])

    # 3. LLMì— ì§ˆì˜ ìƒì„±
    tokenizer, model = load_llm()
    prompt = f"""ë‹¹ì‹ ì€ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” AIì…ë‹ˆë‹¤.
ì•„ë˜ëŠ” ë¬¸ì„œ ë‚´ìš©ì…ë‹ˆë‹¤:

{context}

ì§ˆë¬¸: {user_query}
ë‹µë³€:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=300)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)

    print("ğŸ“ ë‹µë³€:\n", answer)

# ğŸ”¹ ì‹¤í–‰ ì˜ˆì‹œ
if __name__ == "__main__":
    embed_all_documents()
    question = input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”: ")
    answer_question(question)
