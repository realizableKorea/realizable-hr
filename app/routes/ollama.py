from fastapi import FastAPI, Query
from fastapi import APIRouter
import chromadb
import os
from sentence_transformers import SentenceTransformer
import pandas as pd
import pdfplumber
from bs4 import BeautifulSoup


router = APIRouter()
# DB_PATH = "./chroma_db_v3"
DB_PATH = "./chroma_db_v3"
chroma_client = chromadb.PersistentClient(path=DB_PATH)
collection = chroma_client.get_or_create_collection(name="company_docs")

# âœ… ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
# embedding_model = SentenceTransformer("all-MiniLM-L6-v2")
model_4 = 'all-mpnet-base-v2' 
embedding_model = SentenceTransformer(model_4)

def query_embedding(query: str):
    """ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜ """
    return embedding_model.encode(query).tolist()

def search_documents(query: str, top_n: int = 10, min_score: float = 0.3):
    """ ChromaDBì—ì„œ ê°€ì¥ ìœ ì‚¬í•œ ë¬¸ì„œë¥¼ ê²€ìƒ‰ """
    query_vector = query_embedding(query)
    results = collection.query(
        query_embeddings=[query_vector],
        n_results=top_n,
        # include_distances=True
    )
    return results

def generate_response(query: str, results):
    """ ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‘ë‹µ ìƒì„± """
    if not results or "documents" not in results or not results["documents"]:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    # âœ… ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ì—ì„œ `None` ê°’ ì œê±° (documentsëŠ” `list of lists` í˜•íƒœ)
    relevant_docs = [
        doc for doc_list in results["documents"] for doc in doc_list if doc is not None
    ]

    if not relevant_docs:  # ëª¨ë“  ê°’ì´ Noneì´ë©´ ì˜ˆì™¸ ì²˜ë¦¬
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    context = "\n".join(relevant_docs)

    response = f"ğŸ“Œ [HR Bot] ë‹¤ìŒì€ '{query}'ì— ëŒ€í•œ ì •ë³´ì…ë‹ˆë‹¤:\n{context}"
    return response


def extract_text_from_html(file_path):
    """ HTML íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ """
    with open(file_path, "r", encoding="utf-8") as f:
        soup = BeautifulSoup(f, "html.parser")
    return soup.get_text(separator=" ", strip=True)

def extract_text_from_csv(file_path):
    """ CSV íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ """
    try:
        df = pd.read_csv(file_path, encoding="utf-8")
    except UnicodeDecodeError:
        df = pd.read_csv(file_path, encoding="latin-1")

    text_data = df.astype(str).apply(lambda x: ' '.join(x.dropna()), axis=1).tolist()
    return " ".join(text_data)  # CSVì˜ ëª¨ë“  í…ìŠ¤íŠ¸ë¥¼ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨

def extract_text_from_pdf(file_path):
    """ PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ """
    text = []
    with pdfplumber.open(file_path) as pdf:
        for page in pdf.pages:
            text.append(page.extract_text() or "")
    return " ".join(text).strip()

def get_all_files(directory, extensions):
    """ íŠ¹ì • í™•ì¥ìì˜ íŒŒì¼ì„ ì¬ê·€ì ìœ¼ë¡œ ê²€ìƒ‰ """
    matched_files = []
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(extensions):
                matched_files.append(os.path.join(root, file))
    return matched_files

def embed_documents():
    """ HTML, CSV, PDF ë¬¸ì„œë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ê³  ChromaDBì— ì €ì¥ """
    docs_path = "./realizable_internal_data"
    html_files = get_all_files(docs_path, (".html", ".htm"))
    csv_files = get_all_files(docs_path, (".csv",))
    pdf_files = get_all_files(docs_path, (".pdf",))

    all_files = html_files + csv_files + pdf_files
    if not all_files:
        print("âŒ No HTML, CSV, or PDF files found in docs directory!")
        return

    print(f"ğŸ” Found {len(all_files)} files (HTML, CSV, PDF). Processing...")

    for file_path in all_files:
        doc_id = os.path.basename(file_path)  # íŒŒì¼ëª…ì„ IDë¡œ ì‚¬ìš©

        # íŒŒì¼ ìœ í˜•ë³„ë¡œ ì²˜ë¦¬
        if file_path.endswith((".html", ".htm")):
            text = extract_text_from_html(file_path)
        elif file_path.endswith(".csv"):
            text = extract_text_from_csv(file_path)
        elif file_path.endswith(".pdf"):
            text = extract_text_from_pdf(file_path)
        else:
            continue

        if len(text) > 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ì„œëŠ” ì œì™¸
            embedding = embedding_model.encode(text).tolist()
            collection.add(ids=[doc_id], embeddings=[embedding], metadatas=[{"source": file_path}])
            print(f"âœ… Embedded {file_path}")
        else:
            print(f"âš ï¸ Skipping {file_path} (too short)")

@router.get("/ask/")
def ask_hr_bot(question: str = Query(..., title="ì§ˆë¬¸", description="HR ê´€ë ¨ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")):
    """ ì§ˆë¬¸ì— ê¸°ë°˜í•œ ê¸°ì—… ë‚´ë¶€ ì •ë³´ë¥¼ ë‹µë³€ ì œê³µ """
    results = search_documents(question)
    answer = generate_response(question, results)
    return {"question": question, "answer": answer}

@router.get("/count_study_data")
def collection_count():
    """ í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜ ì‘ë‹µ """
    return {"í˜„ì¬ ì €ì¥ëœ ë¬¸ì„œ ê°œìˆ˜": collection.count()}

@router.get("/show_sample_data")
def get_sample():
    """ í•™ìŠµí•œ ë°ì´í„°ì¤‘ ìƒ˜í”Œ 3ê°œë¥¼ ì‘ë‹µí•¨ """
    return {"sample_documents": str(collection.peek(limit=10))}

@router.get("/store_embedding")
def study_start():
    """ embeddingì„ ì‹œì‘í•¨ """
    embed_documents()
    print("collection count: " + str(collection.count()))
    # print("sample_documents below")
    # print(collection.peek(limit=1))
    print("ğŸš€ All documents (HTML, CSV, PDF) embedded into ChromaDB successfully!")
    return {"msg":"ğŸš€ All documents (HTML, CSV, PDF) embedded into ChromaDB successfully!",\
            "embedded data count: ":collection.count()}


def search_docs(query, top_k=5):
    query_embedding = embedding_model.encode(query).tolist()
    results = collection.query(query_embeddings=[query_embedding], n_results=top_k)

    # âœ… ì¤‘ì²© ë¦¬ìŠ¤íŠ¸ë¥¼ í‰íƒ„í™”í•˜ì—¬ 1ì°¨ì› ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    retrieved_docs = sum(results["documents"], [])

    return retrieved_docs

import ollama

def generate_answer_with_rag(query, retrieved_docs):
    # âœ… ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ ë¬¸ì œ í•´ê²°
    context = "\n".join(retrieved_docs)
    
    prompt = f"ë‹¤ìŒ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”:\n\n{context}\n\nì§ˆë¬¸: {query}\në‹µë³€: "
    
    response = ollama.chat(model="mistral", messages=[{"role": "user", "content": prompt}])
    return response['message']['content']

@router.get("/rag")
def rag_api(query: str = Query(..., description="ì‚¬ìš©ì ì§ˆë¬¸")):
    retrieved_docs = search_docs(query)
    response = generate_answer_with_rag(query, retrieved_docs)
    
    return {"answer": response}